{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028b1fa9-04d3-4c44-8fbd-6e4a3e6028d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ONLY HAS TO BE RUN ONCE TO EXPORT DATASET FROM ZIP TO FOLDER\n",
    "import zipfile\n",
    "\n",
    "zip_path = \"Resources.zip\" \n",
    "extract_to = \"Dataset\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79cf06b9-6eaf-4014-b27d-fac97164f3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ONLY HAS TO BE RUN ONCE TO ENABLE 5-FOLD CROSS-VALIDATION\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Making a dictionary to store the disease class of each patient\n",
    "dataset_path = \"Dataset/training\"  # Path to train + validation folder\n",
    "patient_class_dict = {}\n",
    "\n",
    "for patient_folder in os.listdir(dataset_path): # For file in the training folder\n",
    "    if patient_folder.startswith(\".\"):  # Skip hidden folders (.ipynb_checkpoints)\n",
    "        continue\n",
    "\n",
    "    patient_path = os.path.join(dataset_path, patient_folder) # Patient ID, such as patient001\n",
    "\n",
    "    if os.path.isdir(patient_path):  # Process only valid patient folders (skip MANDATATORY_CITATION)\n",
    "        info_file = os.path.join(patient_path, \"Info.cfg\")\n",
    "        with open(info_file, \"r\") as patient_file: # Open file\n",
    "                lines = patient_file.readlines()\n",
    "                patient_class = lines[2].strip()\n",
    "                patient_class_dict[patient_folder] = patient_class # Add to dictionary\n",
    "        \n",
    "# Splitting dataset based on dictionary values (20 patients in each dataset)\n",
    "group_DCM = \"Dataset/group_DCM\"\n",
    "group_HCM = \"Dataset/group_HCM\"\n",
    "group_MINF = \"Dataset/group_MINF\"\n",
    "group_NOR = \"Dataset/group_NOR\"\n",
    "group_RV = \"Dataset/group_RV\"\n",
    "\n",
    "# Create the directories for each class\n",
    "for group in [group_DCM, group_HCM, group_MINF, group_NOR, group_RV]:\n",
    "    if not os.path.exists(group):\n",
    "        os.makedirs(group)\n",
    "\n",
    "# Loop through all patients\n",
    "for patient_folder, disease in patient_class_dict.items():\n",
    "    patient_path = os.path.join(dataset_path, patient_folder)\n",
    "\n",
    "    # Check if the folder exists and it's a directory\n",
    "    if os.path.isdir(patient_path):\n",
    "        # Determine the target group based on disease\n",
    "        if \"DCM\" in disease:\n",
    "            target_group = group_DCM\n",
    "        elif \"HCM\" in disease:\n",
    "            target_group = group_HCM\n",
    "        elif \"MINF\" in disease:\n",
    "            target_group = group_MINF\n",
    "        elif \"NOR\" in disease:\n",
    "            target_group = group_NOR\n",
    "        elif \"RV\" in disease:\n",
    "            target_group = group_RV\n",
    "        else:\n",
    "            print('unknown class error')\n",
    "            continue\n",
    "\n",
    "        # Create the patient's folder inside the target group directory\n",
    "        target_patient_folder = os.path.join(target_group, patient_folder)\n",
    "        if not os.path.exists(target_patient_folder):\n",
    "            os.makedirs(target_patient_folder)\n",
    "\n",
    "        # Copy respective files to new folder\n",
    "        for file_name in os.listdir(patient_path):\n",
    "            file_path = os.path.join(patient_path, file_name)\n",
    "            if os.path.isfile(file_path):  # Check if it's a file\n",
    "                # Move the file to the respective patient folder in the group folder\n",
    "                shutil.copy(file_path, os.path.join(target_patient_folder, file_name)) # YOU CAN ONLY RUN THIS ONCE, AFTER THAT THE TRAINING SET IS EMPTY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57250931-bc04-45b8-9d33-be83f39cb578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 17:13:57.174759: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-12 17:13:57.211732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-12 17:13:57.211752: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-12 17:13:57.211772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-12 17:13:57.218965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-12 17:13:58.025488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Importing!\n"
     ]
    }
   ],
   "source": [
    "## ALL FUNCTIONs NEEDED TO TRAIN AND VALIDATE MODEL\n",
    "import os\n",
    "import shutil\n",
    "data_path_train = \"Dataset/new_training\"\n",
    "data_path_valid = \"Dataset/new_validation\"\n",
    "data_path_test = \"Dataset/testing\"\n",
    "\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import monai\n",
    "import torch.nn.functional as F\n",
    "from medpy.metric.binary import hd, dc\n",
    "import time\n",
    "import torch\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Start up wandb and start logging\n",
    "import wandb\n",
    "print(\"Done Importing!\")\n",
    "\n",
    "def get_ed_es_frames(config_path):\n",
    "    \"\"\"Extract ED and ES frame numbers from the info.cfg file.\"\"\"\n",
    "\n",
    "    ed_frame, es_frame = None, None\n",
    "    with open(config_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('ED:'):\n",
    "                ed_frame = int(line.split(':')[1].strip())\n",
    "            elif line.startswith('ES:'):\n",
    "                es_frame = int(line.split(':')[1].strip())\n",
    "    return ed_frame, es_frame\n",
    "\n",
    "\n",
    "def build_dict_acdc(data_path, mode='train'):\n",
    "    \"\"\"\n",
    "    This function returns a list of dictionaries, each containing the paths to the 2D slices \n",
    "    of the 3D MRI images and their corresponding masks.\n",
    "    \"\"\"\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n",
    "    \n",
    "    dicts = []\n",
    "    \n",
    "    # Loop over all patient directories\n",
    "    patient_dirs = [d for d in glob.glob(os.path.join(data_path, '*')) if os.path.isdir(d)]\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        patient_id = os.path.basename(patient_dir)\n",
    "        config_path = os.path.join(patient_dir, \"Info.cfg\")\n",
    "        \n",
    "        if not os.path.exists(config_path):\n",
    "            continue\n",
    "        \n",
    "        ed_frame, es_frame = get_ed_es_frames(config_path)\n",
    "        \n",
    "        # Identify the ED and ES image and mask paths\n",
    "        ed_img_path = os.path.join(patient_dir, f\"{patient_id}_frame{ed_frame:02d}.nii.gz\")\n",
    "        ed_mask_path = os.path.join(patient_dir, f\"{patient_id}_frame{ed_frame:02d}_gt.nii.gz\")\n",
    "        es_img_path = os.path.join(patient_dir, f\"{patient_id}_frame{es_frame:02d}.nii.gz\")\n",
    "        es_mask_path = os.path.join(patient_dir, f\"{patient_id}_frame{es_frame:02d}_gt.nii.gz\")\n",
    "        \n",
    "        for img_path, mask_path in [(ed_img_path, ed_mask_path), (es_img_path, es_mask_path)]:\n",
    "            if not os.path.exists(img_path) or not os.path.exists(mask_path):\n",
    "                continue\n",
    "            \n",
    "            # Load the 3D image and mask using nibabel\n",
    "            img_volume = nib.load(img_path).get_fdata()\n",
    "            mask_volume = nib.load(mask_path).get_fdata()\n",
    "            #print(\"Unique values in loaded ground truth mask:\", np.unique(mask_volume))\n",
    "            \n",
    "            # Ensure we have the same number of slices for image and mask\n",
    "            num_slices = img_volume.shape[2]\n",
    "            \n",
    "            # Extract 2D slices\n",
    "            for slice_idx in range(num_slices):\n",
    "                img_slice = img_volume[:, :, slice_idx]\n",
    "                mask_slice = mask_volume[:, :, slice_idx]\n",
    "                \n",
    "                dicts.append({'img': img_slice, 'mask': mask_slice})\n",
    "    \n",
    "    return dicts\n",
    "\n",
    "class LoadHeartData(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads 2D slices of MRI data and their corresponding mask for heart segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img_slice = sample['img']\n",
    "        mask_slice = sample['mask'] \n",
    "        \n",
    "        # Ensure the image and mask are in compatible formats\n",
    "        img_slice = np.array(img_slice, dtype=np.float32)\n",
    "        mask_slice = np.array(mask_slice, dtype=np.uint8) \n",
    "        \n",
    "        # Return the slice and mask with metadata. NOT SURE ABOUT THE METATDATA\n",
    "        return {'img': img_slice, 'mask': mask_slice, 'img_meta_dict': {'affine': np.eye(2)}, \n",
    "                'mask_meta_dict': {'affine': np.eye(2)}}\n",
    "\n",
    "HEADER = [\"Name\", \"Dice LV\", \"Volume LV\", \"Err LV(ml)\",\n",
    "          \"Dice RV\", \"Volume RV\", \"Err RV(ml)\",\n",
    "          \"Dice MYO\", \"Volume MYO\", \"Err MYO(ml)\"]\n",
    "\n",
    "\n",
    "# Functions to process files, directories and metrics aka loss function\n",
    "\n",
    "def metrics(img_gt, img_pred, voxel_size):\n",
    "    \"\"\"\n",
    "    Function to compute the metrics between two segmentation maps given as input.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of metrics in this order, [Dice LV, Volume LV, Err LV(ml),\n",
    "    Dice RV, Volume RV, Err RV(ml), Dice MYO, Volume MYO, Err MYO(ml)]\n",
    "    \"\"\"\n",
    "\n",
    "    if img_gt.ndim != img_pred.ndim:\n",
    "        raise ValueError(\"The arrays 'img_gt' and 'img_pred' should have the \"\n",
    "                         \"same dimension, {} against {}\".format(img_gt.ndim,\n",
    "                                                                img_pred.ndim))\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    # Loop on each classes of the input images\n",
    "    for c in [3, 1, 2]:\n",
    "        # Copy the gt image to not alterate the input\n",
    "        gt_c_i = np.copy(img_gt)\n",
    "        gt_c_i[gt_c_i != c] = 0\n",
    "\n",
    "        # Copy the pred image to not alterate the input\n",
    "        pred_c_i = np.copy(img_pred)\n",
    "        pred_c_i[pred_c_i != c] = 0\n",
    "\n",
    "        # Clip the value to compute the volumes\n",
    "        gt_c_i = np.clip(gt_c_i, 0, 1)\n",
    "        pred_c_i = np.clip(pred_c_i, 0, 1)\n",
    "\n",
    "        # Compute the Dice\n",
    "        dice = dc(gt_c_i, pred_c_i)\n",
    "\n",
    "        # Compute volume\n",
    "        volpred = pred_c_i.sum() * np.prod(voxel_size) / 1000.\n",
    "        volgt = gt_c_i.sum() * np.prod(voxel_size) / 1000.\n",
    "\n",
    "        res += [dice, volpred, volpred-volgt]\n",
    "\n",
    "    return res\n",
    "\n",
    "def compute_metrics_on_files(path_gt, path_pred):\n",
    "    \"\"\"\n",
    "    Function to give the metrics for two files\n",
    "\n",
    "    \"\"\"\n",
    "    gt, _, header = load_nii(path_gt)\n",
    "    pred, _, _ = load_nii(path_pred)\n",
    "    zooms = header.get_zooms()\n",
    "\n",
    "    name = os.path.basename(path_gt)\n",
    "    name = name.split('.')[0]\n",
    "    res = metrics(gt, pred, zooms)\n",
    "    res = [\"{:.3f}\".format(r) for r in res]\n",
    "\n",
    "    formatting = \"{:>14}, {:>7}, {:>9}, {:>10}, {:>7}, {:>9}, {:>10}, {:>8}, {:>10}, {:>11}\"\n",
    "    print(formatting.format(*HEADER))\n",
    "    print(formatting.format(name, *res))\n",
    "      \n",
    "# Recombine into a training and a validation set (set 1 to validation and 4 to training)\n",
    "def recombining_data(recombine_index):\n",
    "    \n",
    "    new_train_path = os.path.join(\"Dataset\", 'new_training')\n",
    "    new_val_path = os.path.join(\"Dataset\", 'new_validation')\n",
    "    \n",
    "    # If folder does not exist yet\n",
    "    if not os.path.exists(new_train_path):\n",
    "        os.makedirs(new_train_path)  # Creates the new training folder\n",
    "    if not os.path.exists(new_val_path):\n",
    "        os.makedirs(new_val_path)  # Creates the new validation folder\n",
    "    \n",
    "    # Empty the new_validation folder\n",
    "    if os.path.exists(new_val_path):\n",
    "        shutil.rmtree(new_val_path)\n",
    "        os.makedirs(new_val_path)\n",
    "\n",
    "    # Empty the new_training folder\n",
    "    if os.path.exists(new_train_path):\n",
    "        shutil.rmtree(new_train_path)\n",
    "        os.makedirs(new_train_path)\n",
    "    \n",
    "    val_id = [1,2,3,4]\n",
    "    offset = (recombine_index - 1) * 4\n",
    "    val_id = [element + offset for element in val_id] # Add the offset to each element of val_id\n",
    "    \n",
    "    train_id = list(range(1,21))\n",
    "    for element in val_id:\n",
    "        train_id.remove(element) # remove the validation patients\n",
    "    \n",
    "    # now there is a list of numbers for who should be in val, and who should be in train\n",
    "    \n",
    "    # Define the classes (group folders) you want to loop through\n",
    "    class_folders = ['group_DCM', 'group_HCM', 'group_MINF', 'group_NOR', 'group_RV']\n",
    "    \n",
    "    # Loop through each class folder\n",
    "    for class_folder in class_folders:\n",
    "        class_folder_path = os.path.join(\"Dataset\", class_folder)\n",
    "        \n",
    "        patients_in_class = [folder for folder in os.listdir(class_folder_path)] # list of all file names in class\n",
    "        \n",
    "        for val_target in val_id: # copy all validation patients\n",
    "            val_patient_target = patients_in_class[val_target - 1] # get name of validation patient\n",
    "            \n",
    "            # copy from source to destination\n",
    "            source_folder = os.path.join(\"Dataset\", class_folder,val_patient_target)\n",
    "            destination_folder = os.path.join(new_val_path, val_patient_target)\n",
    "            shutil.copytree(source_folder, destination_folder)\n",
    "            \n",
    "        for train_target in train_id: # copy all training patients\n",
    "            train_patient_target = patients_in_class[train_target - 1] # get name of training patient\n",
    "            \n",
    "            # copy from source to destination\n",
    "            source_folder = os.path.join(\"Dataset\", class_folder,train_patient_target)\n",
    "            destination_folder = os.path.join(new_train_path, train_patient_target)\n",
    "            shutil.copytree(source_folder, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd65ec1f-f089-48b6-90c5-6bbc14d39368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning the loop\n",
      "Beginning loop for index:1\n",
      "Recombining Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk-h-leussink\u001b[0m (\u001b[33mk-h-leussink-university-of-twente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dlmia-course/notebooks/Tutorial1_NumpySimpleITK/DLMIA/wandb/run-20250412_171441-yvyc7nnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/yvyc7nnz' target=\"_blank\">PreProc_2DUNet_Cross_validation_run_1</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/yvyc7nnz' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/yvyc7nnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1506/1506 [00:16<00:00, 94.00it/s]\n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:12<00:00, 89.14it/s]\n",
      "Loading dataset: 100%|██████████| 396/396 [00:04<00:00, 83.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.3.1+cu118\n",
      "---------- Epoch 1/10 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1443: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1443: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1443: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1443: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/monai/data/__init__.py:118: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.7945, time: 36.00 sec\n",
      "Validation metrics: [ 0.57303188  0.80832544  0.22182371  0.02249191 18.50062737 17.94992503\n",
      "  0.33348235  2.63085937  2.01834359]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7241, time: 36.18 sec\n",
      "Validation metrics: [ 0.74308897  0.65273832  0.06623658  0.02307334 19.13887311 18.58817077\n",
      "  0.57596801  1.10238321  0.48986742]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6241, time: 36.72 sec\n",
      "Validation metrics: [ 0.77920642  0.65866872  0.07216698  0.02416544 19.3629577  18.81225537\n",
      "  0.67402781  0.82181187  0.20929609]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.5118, time: 36.83 sec\n",
      "Validation metrics: [ 0.79240609  0.6153054   0.02880366  0.02772264 17.33819444 16.78749211\n",
      "  0.69894466  0.83119476  0.21867898]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.4472, time: 37.05 sec\n",
      "Validation metrics: [ 8.45553272e-01  5.79036458e-01 -7.46527778e-03  4.37356061e-02\n",
      "  1.15238123e+01  1.09731100e+01  7.57874092e-01  6.26432292e-01\n",
      "  1.39165088e-02]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.3858, time: 37.04 sec\n",
      "Validation metrics: [ 0.8457473   0.57601799 -0.01048374  0.55513861  0.84079467  0.29009233\n",
      "  0.75442143  0.59608191 -0.01643387]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.2976, time: 37.24 sec\n",
      "Validation metrics: [ 0.83795629  0.61051136  0.02400963  0.60296615  0.74818892  0.19748658\n",
      "  0.76891807  0.60778883 -0.00472696]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.2437, time: 37.15 sec\n",
      "Validation metrics: [ 0.85259434  0.56357323 -0.0229285   0.63559431  0.69054609  0.13984375\n",
      "  0.77757608  0.69719066  0.08467487]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.2120, time: 37.31 sec\n",
      "Validation metrics: [ 0.85555128  0.57281802 -0.01368371  0.63045608  0.53624527 -0.01445707\n",
      "  0.78454576  0.65716146  0.04464568]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.1984, time: 37.17 sec\n",
      "Validation metrics: [0.83770059 0.59055792 0.00405619 0.56681368 0.94740767 0.39670533\n",
      " 0.76902689 0.64273595 0.03022017]\n",
      "Model saved at PreProc_2DUNet_Cross_validation_models/PreProc_2DUNet_Cross_validation_cross_variant_1.pth\n",
      "Test metrics: [0.8201648  0.61779885 0.00197491 0.62220414 1.03760165 0.3963987\n",
      " 0.74310103 0.67770824 0.07644342]\n",
      "Test metrics: [0.8201648  0.61779885 0.00197491 0.62220414 1.03760165 0.3963987\n",
      " 0.74310103 0.67770824 0.07644342]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▅▆▆██████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▅▆▇██████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▁▁▁▁▇███▇</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▄▄▂▁▁▂▁▁▂</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>███▇▅▁▁▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▄▄▂▁▁▂▁▁▂</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>███▇▅▁▁▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇█████</td></tr><tr><td>epoch_loss</td><td>█▇▆▅▄▃▂▂▁▁</td></tr><tr><td>epoch_time_sec</td><td>▁▂▅▅▇▇█▇█▇</td></tr><tr><td>train_step_loss</td><td>████▇▇▇▇▇▇▇▆▇▆▅▅▅▅▅▄▄▄▄▄▃▃▃▂▃▂▂▄▂▂▂▁▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.8377</td></tr><tr><td>Dice_LV_test</td><td>0.82016</td></tr><tr><td>Dice_MY0</td><td>0.76903</td></tr><tr><td>Dice_MY0_test</td><td>0.7431</td></tr><tr><td>Dice_RV</td><td>0.56681</td></tr><tr><td>Dice_RV_test</td><td>0.6222</td></tr><tr><td>Err_LV</td><td>0.00406</td></tr><tr><td>Err_LV_test</td><td>0.00197</td></tr><tr><td>Err_MY0</td><td>0.03022</td></tr><tr><td>Err_MY0_test</td><td>0.07644</td></tr><tr><td>Err_RV</td><td>0.39671</td></tr><tr><td>Err_RV_test</td><td>0.3964</td></tr><tr><td>Volume_LV</td><td>0.59056</td></tr><tr><td>Volume_LV_test</td><td>0.6178</td></tr><tr><td>Volume_MY0</td><td>0.64274</td></tr><tr><td>Volume_MY0_test</td><td>0.67771</td></tr><tr><td>Volume_RV</td><td>0.94741</td></tr><tr><td>Volume_RV_test</td><td>1.0376</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.19841</td></tr><tr><td>epoch_time_sec</td><td>37.16657</td></tr><tr><td>train_step_loss</td><td>0.10933</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_2DUNet_Cross_validation_run_1</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/yvyc7nnz' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/yvyc7nnz</a><br> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_171441-yvyc7nnz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:2\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dlmia-course/notebooks/Tutorial1_NumpySimpleITK/DLMIA/wandb/run-20250412_172250-fyoxzzg8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/fyoxzzg8' target=\"_blank\">PreProc_2DUNet_Cross_validation_run_2</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/fyoxzzg8' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/fyoxzzg8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1514/1514 [00:16<00:00, 89.56it/s]\n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:12<00:00, 84.45it/s]\n",
      "Loading dataset: 100%|██████████| 388/388 [00:03<00:00, 99.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.3.1+cu118\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8093, time: 36.49 sec\n",
      "Validation metrics: [0.29379113 2.88890142 2.21387323 0.11785909 9.27829414 8.58260712\n",
      " 0.34266392 2.86359939 2.15000805]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7556, time: 36.61 sec\n",
      "Validation metrics: [ 0.6991669   0.79839723  0.12336904  0.04444786 26.76100596 26.06531894\n",
      "  0.55915726  1.24004913  0.5264578 ]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6432, time: 37.11 sec\n",
      "Validation metrics: [ 8.02504122e-01  6.35989852e-01 -3.90383376e-02  3.12886191e-02\n",
      "  3.97714844e+01  3.90757974e+01  6.82427127e-01  8.80223099e-01\n",
      "  1.66631765e-01]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.5117, time: 37.15 sec\n",
      "Validation metrics: [ 7.99651596e-01  5.61420747e-01 -1.13607442e-01  2.88637008e-02\n",
      "  4.42632692e+01  4.35675822e+01  6.94804755e-01  8.16422358e-01\n",
      "  1.02831024e-01]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.4561, time: 37.41 sec\n",
      "Validation metrics: [ 8.31835182e-01  6.25237597e-01 -4.97905928e-02  3.19723843e-02\n",
      "  4.09246416e+01  4.02289546e+01  7.27022285e-01  6.92264014e-01\n",
      " -2.13273196e-02]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.4299, time: 37.36 sec\n",
      "Validation metrics: [ 8.40740845e-01  6.21991785e-01 -5.30364046e-02  3.38032282e-02\n",
      "  3.87506363e+01  3.80549493e+01  7.53460263e-01  7.60446198e-01\n",
      "  4.68548647e-02]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.4092, time: 37.52 sec\n",
      "Validation metrics: [ 8.54694736e-01  6.68198293e-01 -6.82989691e-03  3.81741062e-02\n",
      "  3.44812178e+01  3.37855308e+01  7.67270664e-01  7.08046070e-01\n",
      " -5.54526418e-03]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.3932, time: 37.43 sec\n",
      "Validation metrics: [8.65596814e-01 6.82486308e-01 7.45811856e-03 4.29360779e-02\n",
      " 3.09731999e+01 3.02775129e+01 7.84317546e-01 7.19833280e-01\n",
      " 6.24194588e-03]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.3801, time: 37.49 sec\n",
      "Validation metrics: [ 8.59913101e-01  6.57562822e-01 -1.74653673e-02  4.91725107e-02\n",
      "  2.70375846e+01  2.63418976e+01  7.75719451e-01  7.23240174e-01\n",
      "  9.64884021e-03]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.3712, time: 37.51 sec\n",
      "Validation metrics: [ 8.40987708e-01  6.82260793e-01  7.23260309e-03  5.44447964e-02\n",
      "  2.47782257e+01  2.40825387e+01  7.41133217e-01  6.17574098e-01\n",
      " -9.60172358e-02]\n",
      "Model saved at PreProc_2DUNet_Cross_validation_models/PreProc_2DUNet_Cross_validation_cross_variant_2.pth\n",
      "Test metrics: [ 8.32146390e-01  6.37308318e-01  2.14843750e-02  4.95876391e-02\n",
      "  2.53826484e+01  2.47414455e+01  7.34615471e-01  5.48090439e-01\n",
      " -5.31743727e-02]\n",
      "Test metrics: [ 8.32146390e-01  6.37308318e-01  2.14843750e-02  4.95876391e-02\n",
      "  2.53826484e+01  2.47414455e+01  7.34615471e-01  5.48090439e-01\n",
      " -5.31743727e-02]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▆▇▇██████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▄▆▇▇████▇</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>█▂▁▁▁▁▂▂▃▃</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>▁▄▇█▇▇▆▅▅▄</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>▁▄▇█▇▇▆▅▅▄</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>epoch_loss</td><td>█▇▅▃▂▂▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>▁▂▅▆▇▇█▇██</td></tr><tr><td>train_step_loss</td><td>██▇█▇▇▇▆▅▅▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▃▂▁▅▁▂▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.84099</td></tr><tr><td>Dice_LV_test</td><td>0.83215</td></tr><tr><td>Dice_MY0</td><td>0.74113</td></tr><tr><td>Dice_MY0_test</td><td>0.73462</td></tr><tr><td>Dice_RV</td><td>0.05444</td></tr><tr><td>Dice_RV_test</td><td>0.04959</td></tr><tr><td>Err_LV</td><td>0.00723</td></tr><tr><td>Err_LV_test</td><td>0.02148</td></tr><tr><td>Err_MY0</td><td>-0.09602</td></tr><tr><td>Err_MY0_test</td><td>-0.05317</td></tr><tr><td>Err_RV</td><td>24.08254</td></tr><tr><td>Err_RV_test</td><td>24.74145</td></tr><tr><td>Volume_LV</td><td>0.68226</td></tr><tr><td>Volume_LV_test</td><td>0.63731</td></tr><tr><td>Volume_MY0</td><td>0.61757</td></tr><tr><td>Volume_MY0_test</td><td>0.54809</td></tr><tr><td>Volume_RV</td><td>24.77823</td></tr><tr><td>Volume_RV_test</td><td>25.38265</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.37125</td></tr><tr><td>epoch_time_sec</td><td>37.50645</td></tr><tr><td>train_step_loss</td><td>0.32173</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_2DUNet_Cross_validation_run_2</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/fyoxzzg8' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/fyoxzzg8</a><br> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_172250-fyoxzzg8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:3\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dlmia-course/notebooks/Tutorial1_NumpySimpleITK/DLMIA/wandb/run-20250412_173100-z6koedg0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/z6koedg0' target=\"_blank\">PreProc_2DUNet_Cross_validation_run_3</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/z6koedg0' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/z6koedg0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1538/1538 [00:18<00:00, 83.19it/s]\n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 90.66it/s]\n",
      "Loading dataset: 100%|██████████| 364/364 [00:04<00:00, 82.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.3.1+cu118\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.7954, time: 37.19 sec\n",
      "Validation metrics: [0.0917425  9.54276271 8.88509186 0.39942052 1.25762363 0.7309624\n",
      " 0.43844983 1.96517857 1.32525755]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7045, time: 37.46 sec\n",
      "Validation metrics: [0.42293102 1.63465831 0.97698747 0.4726624  1.06380065 0.53713942\n",
      " 0.61461044 0.92080615 0.28088513]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.5723, time: 37.89 sec\n",
      "Validation metrics: [0.74463322 0.79109718 0.13342634 0.51345054 0.96466346 0.43800223\n",
      " 0.65314792 0.80242102 0.1625    ]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.4147, time: 37.88 sec\n",
      "Validation metrics: [0.80442797 0.67111092 0.01344008 0.56955779 0.75180288 0.22514166\n",
      " 0.69431833 0.72633499 0.08641398]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.3095, time: 38.02 sec\n",
      "Validation metrics: [0.83690016 0.67070742 0.01303657 0.59251457 0.80378606 0.27712483\n",
      " 0.7459018  0.71301082 0.0730898 ]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.2536, time: 38.04 sec\n",
      "Validation metrics: [ 0.84556069  0.66854396  0.01087311  0.62393655  0.5903374   0.06367617\n",
      "  0.74754738  0.63851305 -0.00140797]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.2190, time: 38.10 sec\n",
      "Validation metrics: [ 0.83185974  0.61740213 -0.04026872  0.63942758  0.59310182  0.06644059\n",
      "  0.7576299   0.71149554  0.07157452]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.1994, time: 38.07 sec\n",
      "Validation metrics: [ 0.84196404  0.64360405 -0.01406679  0.63895229  0.60535285  0.07869162\n",
      "  0.75153524  0.59638994 -0.04353108]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.1885, time: 38.18 sec\n",
      "Validation metrics: [0.83729189 0.67203383 0.01436298 0.63879908 0.69290007 0.16623884\n",
      " 0.76572427 0.72547218 0.08555117]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.1724, time: 38.06 sec\n",
      "Validation metrics: [ 0.83749401  0.66970295  0.01203211  0.64370016  0.7045115   0.17785027\n",
      "  0.76152231  0.57677284 -0.06314818]\n",
      "Model saved at PreProc_2DUNet_Cross_validation_models/PreProc_2DUNet_Cross_validation_cross_variant_3.pth\n",
      "Test metrics: [ 0.84064053  0.62287697  0.00705303  0.70723432  0.76425418  0.12305123\n",
      "  0.74876419  0.54926231 -0.0520025 ]\n",
      "Test metrics: [ 0.84064053  0.62287697  0.00705303  0.70723432  0.76425418  0.12305123\n",
      "  0.74876419  0.54926231 -0.0520025 ]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▄▇███████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▅▆▆██████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▃▄▆▇▇████</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▃▂▂▂▁▂▁▂▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>█▆▅▃▃▁▁▁▂▂</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▃▂▂▂▁▂▁▂▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>█▆▅▃▃▁▁▁▂▂</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss</td><td>█▇▅▄▃▂▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>▁▃▆▆▇▇▇▇█▇</td></tr><tr><td>train_step_loss</td><td>███▇▇▇▆▆▅▅▃▂▃▄▂▃▄▄▂▃▃▃▃▄▂▂▂▃▁▁▁▁▁▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.83749</td></tr><tr><td>Dice_LV_test</td><td>0.84064</td></tr><tr><td>Dice_MY0</td><td>0.76152</td></tr><tr><td>Dice_MY0_test</td><td>0.74876</td></tr><tr><td>Dice_RV</td><td>0.6437</td></tr><tr><td>Dice_RV_test</td><td>0.70723</td></tr><tr><td>Err_LV</td><td>0.01203</td></tr><tr><td>Err_LV_test</td><td>0.00705</td></tr><tr><td>Err_MY0</td><td>-0.06315</td></tr><tr><td>Err_MY0_test</td><td>-0.052</td></tr><tr><td>Err_RV</td><td>0.17785</td></tr><tr><td>Err_RV_test</td><td>0.12305</td></tr><tr><td>Volume_LV</td><td>0.6697</td></tr><tr><td>Volume_LV_test</td><td>0.62288</td></tr><tr><td>Volume_MY0</td><td>0.57677</td></tr><tr><td>Volume_MY0_test</td><td>0.54926</td></tr><tr><td>Volume_RV</td><td>0.70451</td></tr><tr><td>Volume_RV_test</td><td>0.76425</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.17243</td></tr><tr><td>epoch_time_sec</td><td>38.06306</td></tr><tr><td>train_step_loss</td><td>0.08956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_2DUNet_Cross_validation_run_3</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/z6koedg0' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/z6koedg0</a><br> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_173100-z6koedg0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:4\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dlmia-course/notebooks/Tutorial1_NumpySimpleITK/DLMIA/wandb/run-20250412_173912-t4b9p6q3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/t4b9p6q3' target=\"_blank\">PreProc_2DUNet_Cross_validation_run_4</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/t4b9p6q3' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/t4b9p6q3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1498/1498 [00:17<00:00, 85.36it/s]\n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:11<00:00, 90.50it/s]\n",
      "Loading dataset: 100%|██████████| 404/404 [00:04<00:00, 91.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.3.1+cu118\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8278, time: 36.32 sec\n",
      "Validation metrics: [ 0.29351888  2.91185412  2.22452429  0.04821212 22.18493193 21.51383045\n",
      "  0.21172881  5.07602104  4.42617188]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7626, time: 36.37 sec\n",
      "Validation metrics: [ 7.74555237e-01  6.32584313e-01 -5.47455136e-02  3.15293556e-02\n",
      "  3.64559367e+01  3.57848352e+01  5.99683236e-01  1.07062577e+00\n",
      "  4.20776609e-01]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6390, time: 36.87 sec\n",
      "Validation metrics: [ 7.84242688e-01  6.62217667e-01 -2.51121597e-02  2.64396097e-02\n",
      "  4.66358447e+01  4.59647432e+01  6.69448623e-01  7.45362778e-01\n",
      "  9.55136139e-02]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.5205, time: 36.80 sec\n",
      "Validation metrics: [ 8.34167107e-01  6.80936726e-01 -6.39310025e-03  2.65457257e-02\n",
      "  4.69712020e+01  4.63001006e+01  7.35072005e-01  7.67539449e-01\n",
      "  1.17690285e-01]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.4694, time: 37.03 sec\n",
      "Validation metrics: [ 8.18311588e-01  5.85852413e-01 -1.01477413e-01  2.87161030e-02\n",
      "  4.42681002e+01  4.35969988e+01  7.17384359e-01  6.51558632e-01\n",
      "  1.70946782e-03]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.4363, time: 36.98 sec\n",
      "Validation metrics: [ 8.36563067e-01  6.51748144e-01 -3.55816832e-02  3.08616604e-02\n",
      "  4.11306002e+01  4.04594988e+01  7.58378079e-01  6.54111231e-01\n",
      "  4.26206683e-03]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.4146, time: 37.18 sec\n",
      "Validation metrics: [8.35611274e-01 6.92481436e-01 5.15160891e-03 3.52029636e-02\n",
      " 3.61002630e+01 3.54291615e+01 7.61869076e-01 6.91452661e-01\n",
      " 4.16034963e-02]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.4024, time: 37.07 sec\n",
      "Validation metrics: [ 0.81810069  0.61299505 -0.07433478  0.03775269 33.21420947 32.54310798\n",
      "  0.75223731  0.75328744  0.10343827]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.3910, time: 37.12 sec\n",
      "Validation metrics: [ 0.84371739  0.63248376 -0.05484607  0.04000497 31.72899907 31.05789759\n",
      "  0.77672053  0.7375      0.08765084]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.3798, time: 37.03 sec\n",
      "Validation metrics: [ 0.85410339  0.65845065 -0.02887918  0.04647532 27.3597811  26.68867961\n",
      "  0.78512002  0.71687036  0.06702119]\n",
      "Model saved at PreProc_2DUNet_Cross_validation_models/PreProc_2DUNet_Cross_validation_cross_variant_4.pth\n",
      "Test metrics: [ 8.20923360e-01  6.12051289e-01 -3.77265335e-03  4.44947254e-02\n",
      "  2.76524018e+01  2.70111989e+01  7.52975085e-01  6.82433492e-01\n",
      "  8.11686803e-02]\n",
      "Test metrics: [ 8.20923360e-01  6.12051289e-01 -3.77265335e-03  4.44947254e-02\n",
      "  2.76524018e+01  2.70111989e+01  7.52975085e-01  6.82433492e-01\n",
      "  8.11686803e-02]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>▁▇▇███████</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▆▇▇▇█████</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>█▃▁▁▂▂▄▅▅▇</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>▁▅██▇▆▅▄▄▂</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>▁▅██▇▆▅▄▄▂</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>epoch_loss</td><td>█▇▅▃▂▂▂▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>▁▁▅▅▇▆█▇█▇</td></tr><tr><td>train_step_loss</td><td>██▇█▇▇▆▆▆▆▅▄▄▃▃▂▂▅▂▂▃▄▁▂▃▂▄▃▁▁▃▂▁▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.8541</td></tr><tr><td>Dice_LV_test</td><td>0.82092</td></tr><tr><td>Dice_MY0</td><td>0.78512</td></tr><tr><td>Dice_MY0_test</td><td>0.75298</td></tr><tr><td>Dice_RV</td><td>0.04648</td></tr><tr><td>Dice_RV_test</td><td>0.04449</td></tr><tr><td>Err_LV</td><td>-0.02888</td></tr><tr><td>Err_LV_test</td><td>-0.00377</td></tr><tr><td>Err_MY0</td><td>0.06702</td></tr><tr><td>Err_MY0_test</td><td>0.08117</td></tr><tr><td>Err_RV</td><td>26.68868</td></tr><tr><td>Err_RV_test</td><td>27.0112</td></tr><tr><td>Volume_LV</td><td>0.65845</td></tr><tr><td>Volume_LV_test</td><td>0.61205</td></tr><tr><td>Volume_MY0</td><td>0.71687</td></tr><tr><td>Volume_MY0_test</td><td>0.68243</td></tr><tr><td>Volume_RV</td><td>27.35978</td></tr><tr><td>Volume_RV_test</td><td>27.6524</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.37984</td></tr><tr><td>epoch_time_sec</td><td>37.02934</td></tr><tr><td>train_step_loss</td><td>0.35956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_2DUNet_Cross_validation_run_4</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/t4b9p6q3' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/t4b9p6q3</a><br> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_173912-t4b9p6q3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loop for index:5\n",
      "Recombining Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/dlmia-course/notebooks/Tutorial1_NumpySimpleITK/DLMIA/wandb/run-20250412_174718-ze43tcg9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/ze43tcg9' target=\"_blank\">PreProc_2DUNet_Cross_validation_run_5</a></strong> to <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/ze43tcg9' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/ze43tcg9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 1552/1552 [00:18<00:00, 86.01it/s]\n",
      "Loading dataset: 100%|██████████| 1076/1076 [00:12<00:00, 88.97it/s]\n",
      "Loading dataset: 100%|██████████| 350/350 [00:04<00:00, 84.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Current Device: 0\n",
      "CUDA Device Name: Tesla T4\n",
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA version (PyTorch): 11.8\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "2.3.1+cu118\n",
      "---------- Epoch 1/10 ----------\n",
      "Epoch 1 average loss: 0.8212, time: 37.52 sec\n",
      "Validation metrics: [1.69090066e-02 3.98451116e+01 3.90230491e+01 1.58038794e-01\n",
      " 7.01166518e+00 6.19663393e+00 4.23886918e-01 2.12006696e+00\n",
      " 1.22376786e+00]\n",
      "---------- Epoch 2/10 ----------\n",
      "Epoch 2 average loss: 0.7667, time: 37.78 sec\n",
      "Validation metrics: [1.23596026e-02 4.56117054e+01 4.47896429e+01 4.85776902e-01\n",
      " 1.52428125e+00 7.09250000e-01 5.82041387e-01 1.25500000e+00\n",
      " 3.58700893e-01]\n",
      "---------- Epoch 3/10 ----------\n",
      "Epoch 3 average loss: 0.6911, time: 38.29 sec\n",
      "Validation metrics: [ 1.50957928e-02  4.98692232e+01  4.90471607e+01  5.85514826e-01\n",
      "  9.26741071e-01  1.11709821e-01  6.64641213e-01  8.86741071e-01\n",
      " -9.55803571e-03]\n",
      "---------- Epoch 4/10 ----------\n",
      "Epoch 4 average loss: 0.5802, time: 38.23 sec\n",
      "Validation metrics: [ 1.31242391e-02  4.99924062e+01  4.91703438e+01  6.00627814e-01\n",
      "  9.80598214e-01  1.65566964e-01  6.85888155e-01  7.84089286e-01\n",
      " -1.12209821e-01]\n",
      "---------- Epoch 5/10 ----------\n",
      "Epoch 5 average loss: 0.5198, time: 38.58 sec\n",
      "Validation metrics: [ 1.45974658e-02  5.01459554e+01  4.93238929e+01  6.47654971e-01\n",
      "  7.62915179e-01 -5.21160714e-02  7.23254622e-01  7.68660714e-01\n",
      " -1.27638393e-01]\n",
      "---------- Epoch 6/10 ----------\n",
      "Epoch 6 average loss: 0.4949, time: 38.39 sec\n",
      "Validation metrics: [ 1.47024122e-02  5.01121518e+01  4.92900893e+01  6.67309009e-01\n",
      "  7.91102679e-01 -2.39285714e-02  7.42344150e-01  7.88883929e-01\n",
      " -1.07415179e-01]\n",
      "---------- Epoch 7/10 ----------\n",
      "Epoch 7 average loss: 0.4823, time: 38.59 sec\n",
      "Validation metrics: [ 1.49501144e-02  5.01742321e+01  4.93521696e+01  6.60093147e-01\n",
      "  6.92691964e-01 -1.22339286e-01  7.34397462e-01  6.93062500e-01\n",
      " -2.03236607e-01]\n",
      "---------- Epoch 8/10 ----------\n",
      "Epoch 8 average loss: 0.4692, time: 38.58 sec\n",
      "Validation metrics: [ 1.45936703e-02  5.01132009e+01  4.92911384e+01  6.73552574e-01\n",
      "  7.47276786e-01 -6.77544643e-02  7.44675938e-01  6.66517857e-01\n",
      " -2.29781250e-01]\n",
      "---------- Epoch 9/10 ----------\n",
      "Epoch 9 average loss: 0.4667, time: 38.64 sec\n",
      "Validation metrics: [ 1.48094813e-02  4.99488170e+01  4.91267545e+01  6.80770557e-01\n",
      "  7.52941964e-01 -6.20892857e-02  7.66437251e-01  7.68504464e-01\n",
      " -1.27794643e-01]\n",
      "---------- Epoch 10/10 ----------\n",
      "Epoch 10 average loss: 0.4645, time: 38.54 sec\n",
      "Validation metrics: [ 1.45877371e-02  4.97383795e+01  4.89163170e+01  6.78417403e-01\n",
      "  7.88718750e-01 -2.63125000e-02  7.52909838e-01  7.50696429e-01\n",
      " -1.45602679e-01]\n",
      "Model saved at PreProc_2DUNet_Cross_validation_models/PreProc_2DUNet_Cross_validation_cross_variant_5.pth\n",
      "Test metrics: [1.09032067e-02 4.99005939e+01 4.92847700e+01 6.82055460e-01\n",
      " 7.36142251e-01 9.49393007e-02 7.29586601e-01 6.46750116e-01\n",
      " 4.54853044e-02]\n",
      "Test metrics: [1.09032067e-02 4.99005939e+01 4.92847700e+01 6.82055460e-01\n",
      " 7.36142251e-01 9.49393007e-02 7.29586601e-01 6.46750116e-01\n",
      " 4.54853044e-02]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>█▁▅▂▄▅▅▄▅▄</td></tr><tr><td>Dice_LV_test</td><td>▁</td></tr><tr><td>Dice_MY0</td><td>▁▄▆▆▇█▇███</td></tr><tr><td>Dice_MY0_test</td><td>▁</td></tr><tr><td>Dice_RV</td><td>▁▅▇▇██████</td></tr><tr><td>Dice_RV_test</td><td>▁</td></tr><tr><td>Err_LV</td><td>▁▅████████</td></tr><tr><td>Err_LV_test</td><td>▁</td></tr><tr><td>Err_MY0</td><td>█▄▂▂▁▂▁▁▁▁</td></tr><tr><td>Err_MY0_test</td><td>▁</td></tr><tr><td>Err_RV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Err_RV_test</td><td>▁</td></tr><tr><td>Volume_LV</td><td>▁▅████████</td></tr><tr><td>Volume_LV_test</td><td>▁</td></tr><tr><td>Volume_MY0</td><td>█▄▂▂▁▂▁▁▁▁</td></tr><tr><td>Volume_MY0_test</td><td>▁</td></tr><tr><td>Volume_RV</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Volume_RV_test</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▆▆▆▆▆▆▇██</td></tr><tr><td>epoch_loss</td><td>█▇▅▃▂▂▁▁▁▁</td></tr><tr><td>epoch_time_sec</td><td>▁▃▆▅█▆███▇</td></tr><tr><td>train_step_loss</td><td>██▇▇▇▇▆▇▇▆▆▆▆▄▅▅▅▃▂▂▂▂▃▂▂▂▂▄▃▂▂▄▂▂▄▅▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dice_LV</td><td>0.01459</td></tr><tr><td>Dice_LV_test</td><td>0.0109</td></tr><tr><td>Dice_MY0</td><td>0.75291</td></tr><tr><td>Dice_MY0_test</td><td>0.72959</td></tr><tr><td>Dice_RV</td><td>0.67842</td></tr><tr><td>Dice_RV_test</td><td>0.68206</td></tr><tr><td>Err_LV</td><td>48.91632</td></tr><tr><td>Err_LV_test</td><td>49.28477</td></tr><tr><td>Err_MY0</td><td>-0.1456</td></tr><tr><td>Err_MY0_test</td><td>0.04549</td></tr><tr><td>Err_RV</td><td>-0.02631</td></tr><tr><td>Err_RV_test</td><td>0.09494</td></tr><tr><td>Volume_LV</td><td>49.73838</td></tr><tr><td>Volume_LV_test</td><td>49.90059</td></tr><tr><td>Volume_MY0</td><td>0.7507</td></tr><tr><td>Volume_MY0_test</td><td>0.64675</td></tr><tr><td>Volume_RV</td><td>0.78872</td></tr><tr><td>Volume_RV_test</td><td>0.73614</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>epoch_loss</td><td>0.46449</td></tr><tr><td>epoch_time_sec</td><td>38.54258</td></tr><tr><td>train_step_loss</td><td>0.39718</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PreProc_2DUNet_Cross_validation_run_5</strong> at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project/runs/ze43tcg9' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project/runs/ze43tcg9</a><br> View project at: <a href='https://wandb.ai/DLMI_Project/DLMI_Project' target=\"_blank\">https://wandb.ai/DLMI_Project/DLMI_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_174718-ze43tcg9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ENTIRE TRAINING AND MODEL SAVING LOOP, sending all training, validation and test data to wandb\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    ScaleIntensityd,\n",
    "    Spacingd,\n",
    "    ResizeWithPadOrCropd,\n",
    "    EnsureTyped,\n",
    "    RandZoomd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    ")\n",
    "\n",
    "experiment_name = \"PreProc_2DUNet_Cross_validation\" # CHANGE THIS PER RUN!\n",
    "\n",
    "# Define folder based on the WandB run name and create the folder.\n",
    "folder_save_path = experiment_name +  \"_models\"\n",
    "os.makedirs(folder_save_path, exist_ok=True)\n",
    "\n",
    "print(\"Beginning the loop\")\n",
    "# Recombine dataset\n",
    "recombine_index = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Beginning Model train loop\n",
    "for idx in recombine_index:\n",
    "    print(\"Beginning loop for index:\" + str(idx))\n",
    "    # Recombine new validation and training set (in folders)\n",
    "    print(\"Recombining Data\")\n",
    "    recombining_data(idx)\n",
    "\n",
    "    # Initialize a new WandB run with configuration based on the experiment name.\n",
    "    run = wandb.init(\n",
    "        entity=\"DLMI_Project\",\n",
    "        project=\"DLMI_Project\",\n",
    "        config={\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"architecture\": experiment_name,  # Using experiment name as the architecture identifier\n",
    "            \"dataset\": \"ACDC\",\n",
    "            \"epochs\": 10, # Adjust to e.g. 80 for a proper training\n",
    "        },\n",
    "        name=f\"{experiment_name}_run_{idx}\"\n",
    "    )\n",
    "\n",
    "    # Combine the folder path with the model filename.\n",
    "    model_save_path = os.path.join(folder_save_path, f\"{experiment_name}_cross_variant_{idx}.pth\")\n",
    "    \n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "\n",
    "    # Define a common preprocessing pipeline (containing the deterministic transforms)\n",
    "    common_transform = Compose([\n",
    "        LoadHeartData(),  \n",
    "        AddChanneld(keys=[\"img\", \"mask\"]), # Add channel dimension\n",
    "        ScaleIntensityd(keys=[\"img\"], minv=0, maxv=1),  # Normalize intensity\n",
    "        Spacingd(keys=[\"img\", \"mask\"], pixdim=(1.25, 1.25), mode=(\"bilinear\", \"nearest\")), # Resample voxel spacing in x and y\n",
    "        ResizeWithPadOrCropd(keys=[\"img\", \"mask\"], spatial_size=[256, 256]), # Ensures all images have the same dimensions (without getting stretched out). \n",
    "        EnsureTyped(keys=[\"img\", \"mask\"])\n",
    "    ])\n",
    "\n",
    "    # Train Transform\n",
    "    train_transforms = Compose([\n",
    "        *common_transform.transforms,  # Apply all common steps first\n",
    "        RandZoomd(keys=[\"img\", \"mask\"], prob=0.1, min_zoom=0.9, max_zoom=1.1, keep_size=True), # Random zoom, not too much so that you don't remove important parts\n",
    "        RandFlipd(keys=[\"img\", \"mask\"], prob=0.1, spatial_axis=0),  # Random flip. Spatial axis=0 for up-down flipping. Left-right flipping is not good because the model has to distinguish the left and right ventricle\n",
    "        RandRotated(keys=[\"img\", \"mask\"], range_x=np.pi/12, prob=0.1, mode=(\"bilinear\", \"nearest\")), # Random rotation for max 15 degrees\n",
    "    ])\n",
    "\n",
    "    # Validation and test transforms (only containing the deterministic transforms)\n",
    "    test_transforms = common_transform\n",
    "    valid_transforms = common_transform\n",
    "\n",
    "    train_data = build_dict_acdc(data_path_train, mode='train')\n",
    "    test_data = build_dict_acdc(data_path_test, mode='test')\n",
    "    valid_data = build_dict_acdc(data_path_valid, mode='val')\n",
    "\n",
    "    # Create CacheDatasets for training and testing\n",
    "    train_dataset = CacheDataset(data=train_data, transform=train_transforms)\n",
    "    test_dataset = CacheDataset(data=test_data, transform=test_transforms)\n",
    "    valid_dataset = CacheDataset(data=valid_data, transform=test_transforms)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "    # DEFINE THE ARCHITECTURE\n",
    "    # ---------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Define the device to use\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "    print(\"CUDA Current Device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No GPU\")\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize the 2D U-Net model\n",
    "    model = UNet(\n",
    "        spatial_dims=2, # 2D \n",
    "        in_channels=1, # Grayscale\n",
    "        out_channels=4, # Multi-label segmentation\n",
    "        channels=(64, 128, 256, 512, 1024),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "    ).to(device)\n",
    "\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    # Define the loss function and optimizer.\n",
    "    loss_function = DiceLoss(softmax=True) # DiceLoss with softmax=True for multilabel segmentation.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate) \n",
    "\n",
    "    # (Optional) DiceMetric for evaluation during training\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # START THE TRAINING\n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = wandb.config.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 10, f\"Epoch {epoch + 1}/{num_epochs}\", \"-\" * 10)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = batch_data[\"img\"].to(device)\n",
    "            # Convert labels from shape (B, 1, H, W) to (B, H, W)\n",
    "            labels = batch_data[\"mask\"].squeeze(1).to(device)\n",
    "            unique_values = torch.unique(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # shape: (B, 4, H, W)\n",
    "            outputs = outputs.contiguous()\n",
    "            # Convert labels to one-hot encoding: shape becomes (B, H, W, 4)\n",
    "            one_hot_labels = F.one_hot(labels.long(), num_classes=4)\n",
    "            # Permute to get shape (B, 4, H, W)\n",
    "            one_hot_labels = one_hot_labels.permute(0, 3, 1, 2).float()\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(outputs, one_hot_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            wandb.log({\"train_step_loss\": loss.item(), \"epoch\": epoch + 1})\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}, time: {epoch_time:.2f} sec\")\n",
    "\n",
    "        # Validation\n",
    "        voxel_size = (1.25, 1.25)  # For 2D slices; adjust as needed\n",
    "\n",
    "        model.eval()\n",
    "        all_metrics = []\n",
    "        with torch.no_grad():\n",
    "            for val_data in valid_loader: # switch to validation loader\n",
    "                val_inputs = val_data[\"img\"].to(device)\n",
    "                val_labels = val_data[\"mask\"].squeeze(1).to(device)  # shape: (B, H, W)\n",
    "                val_outputs = model(val_inputs)  # shape: (B, 4, H, W)\n",
    "\n",
    "                # For evaluation, use the integer label maps directly.\n",
    "                pred_labels = torch.argmax(torch.softmax(val_outputs, dim=1), dim=1)  # (B, H, W)\n",
    "                gt_labels = val_labels  # already in (B, H, W) after squeeze\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                pred_labels_np = pred_labels.cpu().numpy()\n",
    "                gt_labels_np = gt_labels.cpu().numpy()\n",
    "\n",
    "                for gt, pred in zip(gt_labels_np, pred_labels_np):\n",
    "                    sample_metrics = metrics(gt, pred, voxel_size)\n",
    "                    all_metrics.append(sample_metrics)\n",
    "\n",
    "            avg_metrics = np.mean(all_metrics, axis=0)\n",
    "            print(\"Validation metrics:\", avg_metrics)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"epoch_loss\": epoch_loss,\n",
    "                \"Dice_LV\": avg_metrics[0],\n",
    "                \"Volume_LV\": avg_metrics[1],\n",
    "                \"Err_LV\": avg_metrics[2],\n",
    "                \"Dice_RV\": avg_metrics[3],\n",
    "                \"Volume_RV\": avg_metrics[4],\n",
    "                \"Err_RV\": avg_metrics[5],\n",
    "                \"Dice_MY0\": avg_metrics[6],\n",
    "                \"Volume_MY0\": avg_metrics[7],\n",
    "                \"Err_MY0\": avg_metrics[8],\n",
    "                \"epoch_time_sec\": epoch_time\n",
    "            })\n",
    "    \n",
    "\n",
    "    # Save the trained model at the end\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "    \n",
    "    # Get test results\n",
    "    model.eval()\n",
    "    all_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for test_data in test_loader:  \n",
    "            test_inputs = test_data[\"img\"].to(device)\n",
    "            test_labels = test_data[\"mask\"].squeeze(1).to(device)  # shape: (B, H, W)\n",
    "            test_outputs = model(test_inputs)  # shape: (B, 4, H, W)\n",
    "\n",
    "            # For evaluation, use the integer label maps directly.\n",
    "            pred_labels = torch.argmax(torch.softmax(test_outputs, dim=1), dim=1)  # (B, H, W)\n",
    "            gt_labels = test_labels  # already in (B, H, W) after squeeze\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            pred_labels_np = pred_labels.cpu().numpy()\n",
    "            gt_labels_np = gt_labels.cpu().numpy()\n",
    "\n",
    "            for gt, pred in zip(gt_labels_np, pred_labels_np):\n",
    "                sample_metrics = metrics(gt, pred, voxel_size)\n",
    "                all_metrics.append(sample_metrics)\n",
    "\n",
    "        avg_metrics = np.mean(all_metrics, axis=0)\n",
    "        print(\"Test metrics:\", avg_metrics)\n",
    "        wandb.log({\n",
    "            \"Dice_LV_test\": avg_metrics[0],\n",
    "            \"Volume_LV_test\": avg_metrics[1],\n",
    "            \"Err_LV_test\": avg_metrics[2],\n",
    "            \"Dice_RV_test\": avg_metrics[3],\n",
    "            \"Volume_RV_test\": avg_metrics[4],\n",
    "            \"Err_RV_test\": avg_metrics[5],\n",
    "            \"Dice_MY0_test\": avg_metrics[6],\n",
    "            \"Volume_MY0_test\": avg_metrics[7],\n",
    "            \"Err_MY0_test\": avg_metrics[8]\n",
    "        })\n",
    "\n",
    "        print(\"Test metrics:\", avg_metrics)\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6459a-01a1-47bb-9e98-3683165176b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
